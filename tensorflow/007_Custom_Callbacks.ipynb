{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2e8fac7-5e9c-4096-a4ac-0b833f531ced",
   "metadata": {},
   "source": [
    "## Dataset import and exploration\n",
    "- https://www.kaggle.com/shelvigarg/wine-quality-dataset\n",
    "- Refer to https://github.com/fenago/deeplearning/blob/main/tensorflow/003_TensorFlow_Classification.ipynb for detailed preparation instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8d92c7-2972-4552-bf94-d01bb0e075b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (24, 6)\n",
    "plt.rcParams['axes.spines.top'] = False\n",
    "plt.rcParams['axes.spines.right'] = False\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "df = pd.read_csv('data/winequalityN.csv')\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a529677d-9ec9-42a1-b872-8f95602af3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# Prepare the data\n",
    "df = df.dropna()\n",
    "df['is_white_wine'] = [1 if typ == 'white' else 0 for typ in df['type']]\n",
    "df['is_good_wine'] = [1 if quality >= 6 else 0 for quality in df['quality']]\n",
    "df.drop(['type', 'quality'], axis=1, inplace=True)\n",
    "\n",
    "# Train/test split\n",
    "X = df.drop('is_good_wine', axis=1)\n",
    "y = df['is_good_wine']\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c6b768-2339-4612-bc1f-e44fdb68fe6e",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Modelling\n",
    "- Let's declare a function that builds and trains the model\n",
    "- We're doing this because we'll train the exact same model multiple times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0cae3b-3c4f-497e-8e22-c425c5dc8d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d11393-3eae-437d-9e90-67ed11274894",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_and_train(callbacks: list, num_epochs: int = 5) -> tf.keras.Sequential:\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        loss=tf.keras.losses.binary_crossentropy,\n",
    "        optimizer=tf.keras.optimizers.Adam(),\n",
    "        metrics=[tf.keras.metrics.BinaryAccuracy(name='accuracy')]\n",
    "    )\n",
    "\n",
    "    model.fit(\n",
    "        X_train_scaled, \n",
    "        y_train, \n",
    "        epochs=num_epochs,\n",
    "        validation_data=(X_test_scaled, y_test),\n",
    "        callbacks=callbacks,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e6cc25-c6df-43d1-b6a6-6602dad60a01",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Basic custom callback\n",
    "- We'll define what happens on:\n",
    "    - **Train begin** - we'll just print the time at which the training started\n",
    "    - **Train end** - we'll print the time at which the training finsihed, how much time did the training last, and evaluation metrics (accuracy, precision, recall, f1) on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80448e44-45c0-4ee9-8dc1-94e69602f128",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self):\n",
    "        self.time_started = None\n",
    "        self.time_finished = None\n",
    "        \n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.time_started = datetime.now()\n",
    "        print(f'TRAINING STARTED | {self.time_started}\\n')\n",
    "        \n",
    "    def on_train_end(self, logs=None):\n",
    "        self.time_finished = datetime.now()\n",
    "        train_duration = str(self.time_finished - self.time_started)\n",
    "        print(f'\\nTRAINING FINISHED | {self.time_finished} | Duration: {train_duration}')\n",
    "        \n",
    "        tl = f\"Training loss:       {logs['loss']:.5f}\"\n",
    "        ta = f\"Training accuracy:   {logs['accuracy']:.5f}\"\n",
    "        vl = f\"Validation loss:     {logs['val_loss']:.5f}\"\n",
    "        va = f\"Validation accuracy: {logs['val_accuracy']:.5f}\"\n",
    "        \n",
    "        print('\\n'.join([tl, vl, ta, va]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe2a6b8-c7f9-46c7-b32d-18b050292ff5",
   "metadata": {},
   "source": [
    "- Pass in the callback like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31343da9-95de-4e23-b83f-e6d1bb9d396c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_and_train(\n",
    "    callbacks=[MyCallback()]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634ca4bb-cae9-4e41-af2c-a8218369d67b",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Extending the callback functionality\n",
    "- We'll also modify the behavior for a single epoch:\n",
    "    - **Epoch begin** - just save the time to the constructor\n",
    "    - **Epoch end** - Calculate epoch duration and keep track of the training and validation metrics. We'll print them in a somewhat of a visually apealing way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f8cf2b-353e-4f7b-8a77-f0c31e2b439d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self):\n",
    "        self.time_started = None\n",
    "        self.time_finished = None\n",
    "        self.time_curr_epoch = None\n",
    "        \n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.time_started = datetime.now()\n",
    "        print(f'TRAINING STARTED | {self.time_started}\\n')\n",
    "        \n",
    "    def on_train_end(self, logs=None):\n",
    "        self.time_finished = datetime.now()\n",
    "        train_duration = str(self.time_finished - self.time_started)\n",
    "        print(f'\\nTRAINING FINISHED | {self.time_finished} | Duration: {train_duration}')\n",
    "        \n",
    "        tl = f\"Training loss:       {logs['loss']:.5f}\"\n",
    "        ta = f\"Training accuracy:   {logs['accuracy']:.5f}\"\n",
    "        vl = f\"Validation loss:     {logs['val_loss']:.5f}\"\n",
    "        va = f\"Validation accuracy: {logs['val_accuracy']:.5f}\"\n",
    "        \n",
    "        print('\\n'.join([tl, vl, ta, va]))\n",
    "        \n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        self.time_curr_epoch = datetime.now()\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        epoch_dur = (datetime.now() - self.time_curr_epoch).total_seconds()\n",
    "        tl = logs['loss']\n",
    "        ta = logs['accuracy']\n",
    "        vl = logs['val_loss']\n",
    "        va = logs['val_accuracy']\n",
    "        \n",
    "        train_metrics = f\"train_loss: {tl:.5f}, train_accuracy: {ta:.5f}\"\n",
    "        valid_metrics = f\"valid_loss: {vl:.5f}, valid_accuracy: {va:.5f}\"\n",
    "        \n",
    "        print(f\"Epoch: {epoch:4} | Runtime: {epoch_dur:.3f}s | {train_metrics} | {valid_metrics}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9461421-34db-4962-bb81-8fd455003d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_and_train(\n",
    "    callbacks=[MyCallback()]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ac5e00-2e85-4d4d-bc5d-75a73caaf4e2",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Tweaking the functionality even further\n",
    "- We'll declare a function that plots training loss vs. validation loss and training accuracy vs. validation accuracy (`_plot_model_performance()``\n",
    "- We'll plot ot on training end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661e0cfe-7419-4e39-a513-399851a2e7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self):\n",
    "        self.time_started = None\n",
    "        self.time_finished = None\n",
    "        self.time_curr_epoch = None\n",
    "        self.num_epochs = 0\n",
    "        self._loss, self._acc, self._val_loss, self._val_acc = [], [], [], []\n",
    "        \n",
    "    def _plot_model_performance(self):\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "        fig.suptitle('Model performance', size=20)\n",
    "        \n",
    "        ax1.plot(range(self.num_epochs), self._loss, label='Training loss')\n",
    "        ax1.plot(range(self.num_epochs), self._val_loss, label='Validation loss')\n",
    "        ax1.set_xlabel('Epoch', size=14)\n",
    "        ax1.set_ylabel('Loss', size=14)\n",
    "        ax1.legend()\n",
    "        \n",
    "        ax2.plot(range(self.num_epochs), self._acc, label='Training accuracy')\n",
    "        ax2.plot(range(self.num_epochs), self._val_acc, label='Validation Accuracy')\n",
    "        ax2.set_xlabel('Epoch', size=14)\n",
    "        ax2.set_ylabel('Accuracy', size=14)\n",
    "        ax2.legend()\n",
    "        \n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.time_started = datetime.now()\n",
    "        print(f'TRAINING STARTED | {self.time_started}\\n')\n",
    "        \n",
    "    def on_train_end(self, logs=None):\n",
    "        self.time_finished = datetime.now()\n",
    "        train_duration = str(self.time_finished - self.time_started)\n",
    "        print(f'\\nTRAINING FINISHED | {self.time_finished} | Duration: {train_duration}')\n",
    "        \n",
    "        tl = f\"Training loss:       {logs['loss']:.5f}\"\n",
    "        ta = f\"Training accuracy:   {logs['accuracy']:.5f}\"\n",
    "        vl = f\"Validation loss:     {logs['val_loss']:.5f}\"\n",
    "        va = f\"Validation accuracy: {logs['val_accuracy']:.5f}\"\n",
    "        \n",
    "        print('\\n'.join([tl, vl, ta, va]))\n",
    "        self._plot_model_performance()\n",
    "        \n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        self.time_curr_epoch = datetime.now()\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        self.num_epochs += 1\n",
    "        epoch_dur = (datetime.now() - self.time_curr_epoch).total_seconds()\n",
    "        tl = logs['loss']\n",
    "        ta = logs['accuracy']\n",
    "        vl = logs['val_loss']\n",
    "        va = logs['val_accuracy']\n",
    "        \n",
    "        self._loss.append(tl); self._acc.append(ta); self._val_loss.append(vl); self._val_acc.append(va)\n",
    "        \n",
    "        train_metrics = f\"train_loss: {tl:.5f}, train_accuracy: {ta:.5f}\"\n",
    "        valid_metrics = f\"valid_loss: {vl:.5f}, valid_accuracy: {va:.5f}\"\n",
    "        \n",
    "        print(f\"Epoch: {epoch:4} | Runtime: {epoch_dur:.3f}s | {train_metrics} | {valid_metrics}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2df21c-3ba6-4727-bd26-339e7df2b30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_and_train(\n",
    "    callbacks=[MyCallback()],\n",
    "    num_epochs=50\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
