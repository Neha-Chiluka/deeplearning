{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset import and exploration\n",
    "- https://www.kaggle.com/shelvigarg/wine-quality-dataset\n",
    "- Refer to https://github.com/fenago/deeplearning/blob/main/tensorflow/003_TensorFlow_Classification.ipynb for detailed preparation instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "df = pd.read_csv('data/winequalityN.csv')\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# Prepare the data\n",
    "df = df.dropna()\n",
    "df['is_white_wine'] = [1 if typ == 'white' else 0 for typ in df['type']]\n",
    "df['is_good_wine'] = [1 if quality >= 6 else 0 for quality in df['quality']]\n",
    "df.drop(['type', 'quality'], axis=1, inplace=True)\n",
    "\n",
    "# Train/test split\n",
    "X = df.drop('is_good_wine', axis=1)\n",
    "y = df['is_good_wine']\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Callbacks list\n",
    "- I like to declare it beforehand\n",
    "\n",
    "### `ModelCheckpoint`\n",
    "- It will save the model locally on the current epoch if it beats the performance on the previous one\n",
    "- The configuration below saves it to a `hdf5` file in the following format:\n",
    "    - `<dir>/model-<epoch>-<accuracy>.hdf5`\n",
    "- Model is saved only if the validation accuracy is higher than on the previous epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath='checkpoints/model-{epoch:02d}-{val_accuracy:.2f}.hdf5',\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `ReduceLROnPlateau`\n",
    "- Basically if a metric (validation loss) doesn't decrease for a number of epochs (10), reduce the learning rate\n",
    "- New learning rate = old learning rate * factor (0.1)\n",
    "    - nlr = 0.01 * 0.1 = 0.001\n",
    "- You can also set the minimum learning rate below the model won't go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb_reducelr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    factor=0.1,\n",
    "    patience=10,\n",
    "    verbose=1,\n",
    "    min_lr=0.00001\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `EarlyStopping`\n",
    "- If a metric (validation accuracy) doesn't increase by some minimum delta (0.001) for a given number of epochs (10) - kill the training process\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb_earlystop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    min_delta=0.001,\n",
    "    patience=10,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `CSVLogger`\n",
    "- Captures model training history and dumps it to a CSV file\n",
    "- Useful for analyzing the performance later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb_csvlogger = tf.keras.callbacks.CSVLogger(\n",
    "    filename='training_log.csv',\n",
    "    separator=',',\n",
    "    append=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "- For simplicity's sake we'll treat test set as a validation set\n",
    "- In real deep learning projects you'll want to have 3 sets: training, validation, and test\n",
    "- We'll tell the model to train for 1000 epochs, but the `EarlyStopping` callback will kill it way before\n",
    "- Specify callbacks in the `fit()` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.binary_crossentropy,\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    metrics=[tf.keras.metrics.BinaryAccuracy(name='accuracy')]\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_scaled, \n",
    "    y_train, \n",
    "    epochs=1000,\n",
    "    validation_data=(X_test_scaled, y_test),\n",
    "    callbacks=[cb_checkpoint, cb_reducelr, cb_earlystop, cb_csvlogger]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Final evaluation\n",
    "- You can now load the best model - it will be the one with the highest epoch number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = tf.keras.models.load_model('checkpoints/model-25-0.80.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Save yourself some time by calling `predict_classes()` instead of `predict()`\n",
    "- It assigns the classes automatically - you don't have to calculate them from probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_preds = np.ravel(best_model.predict_classes(X_test_scaled))\n",
    "best_model_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Evaluate as you normally would"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(accuracy_score(y_test, best_model_preds))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
