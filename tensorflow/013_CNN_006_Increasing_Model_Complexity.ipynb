{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN 6 - Do Larger Model Lead to Better Performance?\n",
    "- Dataset:\n",
    "    - https://www.kaggle.com/shaunthesheep/microsoft-catsvsdogs-dataset\n",
    "- The dataset isn't deep-learning-compatible by default, here's how to preprocess it:\n",
    "- Code: https://github.com/fenago/deeplearning/blob/main/tensorflow/008_CNN_001_Working_With_Image_Data.ipynb\n",
    "\n",
    "**What you should know by now:**\n",
    "- How to preprocess image data\n",
    "- How to load image data from a directory\n",
    "- What's a convolution, pooling, and a fully-connected layer\n",
    "- Categorical vs. binary classification\n",
    "\n",
    "<br>\n",
    "\n",
    "- First things first, let's import the libraries\n",
    "- The models we'll declare today will have more layers than the ones before\n",
    "    - We'll implement individual classes from TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPool2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import BinaryAccuracy\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "try:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I'm using Nvidia RTX 3060 TI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Load in the data\n",
    "- Use `ImageDataGenerator` to convert image matrices to 0-1 range\n",
    "- Load in the images from directories and convert them to 224x224x3\n",
    "- For memory concerns, we'll lower the batch size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1/255.0)\n",
    "valid_datagen = ImageDataGenerator(rescale=1/255.0)\n",
    "\n",
    "train_data = train_datagen.flow_from_directory(\n",
    "    directory='data/train/',\n",
    "    target_size=(224, 224),\n",
    "    class_mode='categorical',\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "valid_data = valid_datagen.flow_from_directory(\n",
    "    directory='data/validation/',\n",
    "    target_size=(224, 224),\n",
    "    class_mode='categorical',\n",
    "    batch_size=32,\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Model 1\n",
    "- Block 1: Conv, Conv, Pool\n",
    "- Block 2: Conv, Conv, Pool\n",
    "- Block 3: Flatten, Dense\n",
    "- Output\n",
    "\n",
    "<br>\n",
    "\n",
    "- We won't mess with the hyperparameters today"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = tf.keras.Sequential([\n",
    "    Conv2D(filters=32, kernel_size=(3, 3), input_shape=(224, 224, 3), activation='relu'),\n",
    "    Conv2D(filters=32, kernel_size=(3, 3), activation='relu'),\n",
    "    MaxPool2D(pool_size=(2, 2), padding='same'),\n",
    "    \n",
    "    Conv2D(filters=64, kernel_size=(3, 3), activation='relu'),\n",
    "    Conv2D(filters=64, kernel_size=(3, 3), activation='relu'),\n",
    "    MaxPool2D(pool_size=(2, 2), padding='same'),\n",
    "    \n",
    "    Flatten(),\n",
    "    Dense(units=128, activation='relu'),\n",
    "    Dense(units=2, activation='softmax')\n",
    "])\n",
    "\n",
    "\n",
    "model_1.compile(\n",
    "    loss=categorical_crossentropy,\n",
    "    optimizer=Adam(),\n",
    "    metrics=[BinaryAccuracy(name='accuracy')]\n",
    ")\n",
    "model_1_history = model_1.fit(\n",
    "    train_data,\n",
    "    validation_data=valid_data,\n",
    "    epochs=10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "- Not bad, but we got 75% accuracy on the validation set in notebook 010\n",
    "- Will adding complexity to the model increase the accuracy?\n",
    "\n",
    "## Model 2\n",
    "- Block 1: Conv, Conv, Pool\n",
    "- Block 2: Conv, Conv, Pool\n",
    "- Block 3: Conv, Conv, Pool\n",
    "- Block 4: Flatten, Dense\n",
    "- Ouput\n",
    "\n",
    "<br>\n",
    "\n",
    "- This artchitecture is a bit of an overkill for our dataset\n",
    "- The model isn't learning at all:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = Sequential([\n",
    "    Conv2D(filters=32, kernel_size=(3, 3), input_shape=(224, 224, 3), activation='relu'),\n",
    "    Conv2D(filters=32, kernel_size=(3, 3), activation='relu'),\n",
    "    MaxPool2D(pool_size=(2, 2), padding='same'),\n",
    "    \n",
    "    Conv2D(filters=64, kernel_size=(3, 3), activation='relu'),\n",
    "    Conv2D(filters=64, kernel_size=(3, 3), activation='relu'),\n",
    "    MaxPool2D(pool_size=(2, 2), padding='same'),\n",
    "    \n",
    "    Conv2D(filters=128, kernel_size=(3, 3), activation='relu'),\n",
    "    Conv2D(filters=128, kernel_size=(3, 3), activation='relu'),\n",
    "    MaxPool2D(pool_size=(2, 2), padding='same'),\n",
    "    \n",
    "    Flatten(),\n",
    "    Dense(units=128, activation='relu'),\n",
    "    Dense(units=2, activation='softmax')\n",
    "])\n",
    "\n",
    "\n",
    "model_2.compile(\n",
    "    loss=categorical_crossentropy,\n",
    "    optimizer=Adam(),\n",
    "    metrics=[BinaryAccuracy(name='accuracy')]\n",
    ")\n",
    "model_2_history = model_2.fit(\n",
    "    train_data,\n",
    "    validation_data=valid_data,\n",
    "    epochs=10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "- When that happens, you can try experimenting with the learning rate and other parameters\n",
    "- Let's dial it down a bit next\n",
    "\n",
    "<br>\n",
    "\n",
    "## Model 3 \n",
    "- Block 1: Conv, Conv, Pool\n",
    "- Block 2: Conv, Conv, Pool\n",
    "- Block 3: Flatten, Dense, Dropout, Dense\n",
    "- Output\n",
    "\n",
    "<br>\n",
    "\n",
    "- The first model was better than the second\n",
    "- We can try adding a dropout layer as a regulizer and tweaking the fully connected layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3 = tf.keras.Sequential([\n",
    "    Conv2D(filters=32, kernel_size=(3, 3), input_shape=(224, 224, 3), activation='relu'),\n",
    "    Conv2D(filters=32, kernel_size=(3, 3), activation='relu'),\n",
    "    MaxPool2D(pool_size=(2, 2), padding='same'),\n",
    "    \n",
    "    Conv2D(filters=64, kernel_size=(3, 3), activation='relu'),\n",
    "    Conv2D(filters=64, kernel_size=(3, 3), activation='relu'),\n",
    "    MaxPool2D(pool_size=(2, 2), padding='same'),\n",
    "    \n",
    "    Flatten(),\n",
    "    Dense(units=512, activation='relu'),\n",
    "    Dropout(rate=0.3),\n",
    "    Dense(units=128),\n",
    "    Dense(units=2, activation='softmax')\n",
    "])\n",
    "\n",
    "model_3.compile(\n",
    "    loss=categorical_crossentropy,\n",
    "    optimizer=Adam(),\n",
    "    metrics=[BinaryAccuracy(name='accuracy')]\n",
    ")\n",
    "\n",
    "model_3_history = model_3.fit(\n",
    "    train_data,\n",
    "    validation_data=valid_data,\n",
    "    epochs=10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "- It made the model worse\n",
    "- More complex model don't necessarily lead to an increase in performance\n",
    "\n",
    "<br>\n",
    "\n",
    "## Conclusion\n",
    "- There you have it - we've been focusing on the wrong thing from the start\n",
    "- Our model architecture in the notebook 010 was solid\n",
    "    - Adding more layers and complexity decreases the predictive power\n",
    "- We should shift our focus to improving the dataset quality\n",
    "- The following notebook will teach you all about **data augmentation**, and you'll see how it increases the power of our model\n",
    "- After that you'll take your models to new heights with **transfer learning**, and you'll see why coming up with custom architectures is a waste of time in most cases"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
